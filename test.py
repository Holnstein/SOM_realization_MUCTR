import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import data_reader
import visualize_test

from SOM import SOM
import os

from config import DATA_FILE_PATH, TRAIN_SIZE, GRID_SIZE, INPUT_DIM


def load_all_results(prefix=""):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –≤—Å–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"""
    try:
        # som = SOM(grid_size=(7, 7), input_dim=5)
        # som.weights = np.load(f"train_data/{prefix}_weights.npy")

        u_matrix = np.load(f"train_data/{prefix}_u_matrix.npy")
        neuron_clusters = np.load(f"train_data/{prefix}_neuron_clusters.npy")
        train_mapping_df = pd.read_csv(f"train_data/{prefix}_mapping.csv")

        print("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—É—á–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
        return train_mapping_df, u_matrix, neuron_clusters
    except FileNotFoundError as e:
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –æ–±—É—á–µ–Ω–∏—è: {e}")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ train_and_save_results.py")
        return None, None, None


def create_custom_visualization(mapping_df, neuron_clusters, cluster_names):
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))

    # 1. –ö–∞—Ä—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–µ–π—Ä–æ–Ω–æ–≤
    im = ax1.imshow(neuron_clusters, cmap='tab10', interpolation='nearest')
    ax1.set_title('–ö–∞—Ä—Ç–∞ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤ –Ω–µ–π—Ä–æ–Ω–æ–≤', fontsize=14, fontweight='bold')
    ax1.invert_yaxis()

    # 2. –°—Ç–æ–ª–±—á–∞—Ç–∞—è –¥–∏–∞–≥—Ä–∞–º–º–∞ —Å –≤–∞—à–∏–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏
    cluster_counts = mapping_df['neuron_cluster'].value_counts().sort_index()
    labels = [cluster_names[i] for i in cluster_counts.index]

    bars = ax2.bar(labels, cluster_counts.values, color=plt.cm.tab10(range(len(cluster_counts))))
    ax2.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º', fontsize=14, fontweight='bold')
    ax2.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç—Ä–∞–Ω')
    ax2.tick_params(axis='x', rotation=45)

    for bar, count in zip(bars, cluster_counts.values):
        ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.3,
                 str(count), ha='center', va='bottom', fontweight='bold')

    plt.tight_layout()
    plt.show()

    # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é
    print("\n" + "=" * 60)
    print("–ö–õ–ê–°–¢–ï–†–´")
    print("=" * 60)
    for cluster_num, name in cluster_names.items():
        countries = mapping_df[mapping_df['neuron_cluster'] == cluster_num]['country'].tolist()
        print(f"\nüè∑Ô∏è  {name} ({len(countries)} —Å—Ç—Ä–∞–Ω):")
        print("   " + ", ".join(countries))


def main():
    csv_file_path = DATA_FILE_PATH
    prefix = "train_som"

    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    train_mapping_df, u_matrix, neuron_clusters = load_all_results(prefix)
    if train_mapping_df is None:
        return

    all_features, all_country_names, feature_columns = data_reader.load_all_data(csv_file_path)
    print(f"–í—Å–µ–≥–æ —Å—Ç—Ä–∞–Ω –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ: {len(all_country_names)}")

    test_start_idx = TRAIN_SIZE
    test_end_idx = len(all_country_names)

    if test_start_idx >= len(all_country_names):
        print("–û—à–∏–±–∫–∞: –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞ –±–æ–ª—å—à–µ –∏–ª–∏ —Ä–∞–≤–Ω–∞ –≤—Å–µ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É.")
        return

    # –û–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
    features_train_raw = all_features[:TRAIN_SIZE]
    country_names_train = all_country_names[:TRAIN_SIZE]

    # –¢–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ
    features_test_raw = all_features[test_start_idx:test_end_idx]
    country_names_test = all_country_names[test_start_idx:test_end_idx]

    print(f"–û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(country_names_train)} —Å—Ç—Ä–∞–Ω (–∏–Ω–¥–µ–∫—Å—ã 0-{TRAIN_SIZE - 1})")
    print(f"–¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {len(country_names_test)} —Å—Ç—Ä–∞–Ω (–∏–Ω–¥–µ–∫—Å—ã {test_start_idx}-{test_end_idx - 1})")

    scaler_path = "train_data/scaler_train.pkl"
    if not os.path.exists(scaler_path):
        print(f"–û—à–∏–±–∫–∞: Scaler –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ {scaler_path}. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ.")
        return
    scaler = data_reader.load_scaler(scaler_path)

    features_test_normalized = data_reader.prepare_test_data(features_test_raw, scaler)

    weights_path = f"train_data/{prefix}_weights.npy"
    if not os.path.exists(weights_path):
        print(f"–û—à–∏–±–∫–∞: –í–µ—Å–∞ SOM –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ –ø—É—Ç–∏ {weights_path}. –û–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª—å –∑–∞–Ω–æ–≤–æ.")
        return
    loaded_weights = np.load(weights_path)

    som = SOM(grid_size=GRID_SIZE, input_dim=INPUT_DIM)
    som.weights = loaded_weights  # –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω—ã—Ö –≤–µ—Å–æ–≤
    print("–û–±—É—á–µ–Ω–Ω–∞—è SOM –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

    print("\n--- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ SOM –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---")
    test_bmus = []
    test_distances = []
    for i, test_sample in enumerate(features_test_normalized):
        bmu_coords, min_dist = som.find_bmu(test_sample)
        test_bmus.append(bmu_coords)
        test_distances.append(min_dist)
        # print(f"–¢–µ—Å—Ç–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä {i} ({country_names_test[i]}): BMU={bmu_coords}, dist={min_dist:.4f}")

    avg_test_distance = np.mean(test_distances)
    print(f"–°—Ä–µ–¥–Ω–µ–µ —Ä–∞—Å—Å—Ç–æ—è–Ω–∏–µ –¥–æ BMU –Ω–∞ –¢–ï–°–¢–û–í–û–ô –≤—ã–±–æ—Ä–∫–µ: {avg_test_distance:.4f}")

    test_mapping_df = pd.DataFrame({
        'country': country_names_test,
        'bmu_i': [bmu[0] for bmu in test_bmus],
        'bmu_j': [bmu[1] for bmu in test_bmus],
        'distance_to_bmu': test_distances
    })

    test_mapping_df['neuron_cluster'] = test_mapping_df.apply(
        lambda row: neuron_clusters[row['bmu_i'], row['bmu_j']], axis=1
    )

    test_mapping_df.to_csv(f"train_data/som_test_mapping.csv", index=False)
    print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ç–µ—Å—Ç–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ train_data/som_test_mapping.csv")

    mapping_df = train_mapping_df

    # –ö–ê–ö–ò–ï –ö–õ–ê–°–¢–ï–†–´ –ë–´–õ–ò –°–û–•–†–ê–ù–ï–ù–´
    print("\n–°–û–•–†–ê–ù–ï–ù–ù–´–ï –ö–õ–ê–°–¢–ï–†–´:")
    for cluster_num in sorted(mapping_df['neuron_cluster'].unique()):
        countries = mapping_df[mapping_df['neuron_cluster'] == cluster_num]['country'].tolist()
        print(f"–ö–ª–∞—Å—Ç–µ—Ä {cluster_num}: {', '.join(countries[:5])}...")

    # 3. –í–í–û–î–ò–ú –ù–ê–ó–í–ê–ù–ò–Ø
    print("\n" + "=" * 50)

    my_cluster_names = {
        2: "2",
        1: "1",
        0: "0",
        3: "3"
    }

    print("–ù–∞–∑–≤–∞–Ω–∏—è –∫–ª–∞—Å—Ç–µ—Ä–æ–≤:")
    for cluster_num, name in my_cluster_names.items():
        print(f"  –ö–ª–∞—Å—Ç–µ—Ä {cluster_num} ‚Üí '{name}'")

    # 4. –°–æ–∑–¥–∞–µ–º –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—é —Å –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
    create_custom_visualization(mapping_df, neuron_clusters, my_cluster_names)

    print("\n--- –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¢–ï–°–¢–û–í–´–• —Å—Ç—Ä–∞–Ω –ø–æ –∫–ª–∞—Å—Ç–µ—Ä–∞–º ---")
    test_cluster_counts = test_mapping_df['neuron_cluster'].value_counts().sort_index()
    print(test_cluster_counts)

    print("\n--- –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–π ---")
    print("–û–±—É—á–∞—é—â–∏–µ:")
    print(train_mapping_df['neuron_cluster'].value_counts().sort_index())
    print("\n–¢–µ—Å—Ç–æ–≤—ã–µ:")
    print(test_cluster_counts)

    visualize_test.visualize_test_results(u_matrix, neuron_clusters, train_mapping_df, test_mapping_df)

if __name__ == "__main__":
    main()